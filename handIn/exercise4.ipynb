{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Clock():\n",
    "    def __init__(self, n=24):\n",
    "        self.time = 0\n",
    "        self.cycles = 0\n",
    "        self.n = n\n",
    "    \n",
    "    def tick(self):\n",
    "        self.time +=1\n",
    "        if self.time >= self.n:\n",
    "            self.time = 0\n",
    "        self.cycles +=1\n",
    "    \n",
    "    def __eq__(self, other):\n",
    "        return self.time == other.time\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return f\"The time is {self.time}:00\"\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "The time is 0:00\n",
      "The time is 1:00\n",
      "The time is 2:00\n",
      "The time is 3:00\n",
      "The time is 4:00\n",
      "The time is 5:00\n",
      "The time is 6:00\n",
      "The time is 7:00\n",
      "The time is 8:00\n",
      "The time is 9:00\n",
      "The time is 10:00\n",
      "The time is 11:00\n",
      "The time is 12:00\n",
      "The time is 13:00\n",
      "The time is 14:00\n",
      "The time is 15:00\n",
      "The time is 16:00\n",
      "The time is 17:00\n",
      "The time is 18:00\n",
      "The time is 19:00\n",
      "The time is 20:00\n",
      "The time is 21:00\n",
      "The time is 22:00\n",
      "The time is 23:00\n",
      "The time is 0:00\n",
      "The time is 1:00\n",
      "The time is 2:00\n",
      "The time is 3:00\n",
      "The time is 4:00\n",
      "The time is 5:00\n",
      "The time is 6:00\n",
      "The time is 7:00\n",
      "The time is 8:00\n",
      "The time is 9:00\n",
      "The time is 10:00\n",
      "The time is 11:00\n",
      "The time is 12:00\n",
      "The time is 13:00\n",
      "The time is 14:00\n",
      "The time is 15:00\n",
      "The time is 16:00\n",
      "The time is 17:00\n",
      "The time is 18:00\n",
      "The time is 19:00\n",
      "The time is 20:00\n",
      "The time is 21:00\n",
      "The time is 22:00\n",
      "The time is 23:00\n",
      "The time is 0:00\n",
      "The time is 1:00\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = Clock()\n",
    "b = Clock()\n",
    "print(a==b)\n",
    "\n",
    "for i in range(50):\n",
    "    print(a)\n",
    "    a.tick()\n",
    "\n",
    "print(a==b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise Set 4: controlling for observable factors and causal trees\n",
    "\n",
    "In this Exercise Set 4 we will try out different techniques for using matching and try an implementation of causal trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "from scipy.stats import ttest_rel, ttest_ind\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## 4.1 Survival and passenger class\n",
    "\n",
    "We revisit a classic dataset: Titanic. We are interested in analyzing whether the passengers on First class had a higher survival probability. \n",
    "\n",
    "The code below loads the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sns.load_dataset('titanic').dropna(subset=['age'])\n",
    "\n",
    "\n",
    "X = pd.get_dummies(df.drop(['pclass','class', 'alive'],axis=1), drop_first=True).astype('float')\n",
    "D = (df['pclass'] < 3).rename('high_class')\n",
    "y = (df['alive']=='yes').astype('float')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.1:** Compute the ATE of not travelling on a 3rd class ticket, assuming the CIA holds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.33159402095021384"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your answer here\n",
    "def ate(y, D):\n",
    "    y_1 = y[D].mean()\n",
    "    y_0 = y[~D].mean()\n",
    "    return y_1-y_0\n",
    "\n",
    "ate(y, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.2:** Compute the share of males, the proportion travelling alone, and the mean age, by treatment status. Then modify the code below to try out coarsened exact matching on `exact_cols = ['age_group', 'alone','sex_male']` with age bins of size 2, 5, 10 and 15 years. \n",
    ">\n",
    "> Comment on the result. Does coarse matching seem like a feasible approach in this \n",
    "```python\n",
    "age_diff =  2\n",
    "X['age_group'] =  (X.age // age_diff)\n",
    "match_count = \\\n",
    "    pd.DataFrame({'treat':X[D].groupby(exact_cols).size(), \n",
    "                  'control':X[~D].groupby(exact_cols).size()})\\            \n",
    "n_obs_matched = int(match_count.dropna().sum().sum())\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "treated age         34.206825\n",
      "sex_male     0.557103\n",
      "alone        0.498607\n",
      "dtype: float64\n",
      "Not treated age         25.140620\n",
      "sex_male     0.712676\n",
      "alone        0.633803\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Your answer here\n",
    "cols = ['age', 'sex_male', 'alone']\n",
    "print(\"treated\", X[D][cols].mean())\n",
    "print(\"Not treated\", X[~D][cols].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          treat  control\n",
      "age_group alone sex_male                \n",
      "0.0       0.0   0.0         NaN      4.0\n",
      "                1.0         6.0      4.0\n",
      "1.0       0.0   0.0         3.0      5.0\n",
      "                1.0         3.0      5.0\n",
      "2.0       0.0   0.0         3.0      5.0\n",
      "...                         ...      ...\n",
      "33.0      1.0   1.0         1.0      NaN\n",
      "35.0      0.0   1.0         1.0      NaN\n",
      "          1.0   1.0         3.0      1.0\n",
      "37.0      1.0   1.0         NaN      1.0\n",
      "40.0      1.0   1.0         1.0      NaN\n",
      "\n",
      "[118 rows x 2 columns]\n",
      "                          treat  control\n",
      "age_group alone sex_male                \n",
      "0.0       0.0   0.0         5.0     12.0\n",
      "                1.0        10.0     13.0\n",
      "1.0       0.0   0.0         4.0      8.0\n",
      "                1.0         1.0      8.0\n",
      "          1.0   0.0         NaN      1.0\n",
      "2.0       0.0   0.0         3.0      4.0\n",
      "                1.0         1.0      5.0\n",
      "          1.0   0.0         NaN      2.0\n",
      "                1.0         NaN      1.0\n",
      "3.0       0.0   0.0        13.0      8.0\n",
      "                1.0         5.0     10.0\n",
      "          1.0   0.0         5.0     10.0\n",
      "                1.0         9.0     26.0\n",
      "4.0       0.0   0.0        14.0      6.0\n",
      "                1.0         7.0      5.0\n",
      "          1.0   0.0        10.0     12.0\n",
      "                1.0        10.0     50.0\n",
      "5.0       0.0   0.0        10.0      8.0\n",
      "                1.0        11.0     11.0\n",
      "          1.0   0.0         7.0      5.0\n",
      "                1.0        18.0     36.0\n",
      "6.0       0.0   0.0        10.0      5.0\n",
      "                1.0        11.0      4.0\n",
      "          1.0   0.0        13.0      5.0\n",
      "                1.0        16.0     31.0\n",
      "7.0       0.0   0.0        12.0      5.0\n",
      "                1.0        11.0      4.0\n",
      "          1.0   0.0         9.0      1.0\n",
      "                1.0        16.0     14.0\n",
      "8.0       0.0   0.0         7.0      4.0\n",
      "                1.0         6.0      6.0\n",
      "          1.0   0.0         7.0      NaN\n",
      "                1.0         6.0     12.0\n",
      "9.0       0.0   0.0         6.0      4.0\n",
      "                1.0         7.0      NaN\n",
      "          1.0   0.0         3.0      1.0\n",
      "                1.0        13.0      7.0\n",
      "10.0      0.0   0.0         9.0      NaN\n",
      "                1.0         7.0      NaN\n",
      "          1.0   0.0         3.0      NaN\n",
      "                1.0         9.0      4.0\n",
      "11.0      0.0   0.0         2.0      NaN\n",
      "                1.0         1.0      NaN\n",
      "          1.0   0.0         4.0      NaN\n",
      "                1.0         7.0      2.0\n",
      "12.0      0.0   0.0         2.0      NaN\n",
      "                1.0         3.0      NaN\n",
      "          1.0   0.0         1.0      1.0\n",
      "                1.0         7.0      1.0\n",
      "13.0      0.0   1.0         1.0      NaN\n",
      "          1.0   1.0         2.0      1.0\n",
      "14.0      0.0   1.0         1.0      NaN\n",
      "          1.0   1.0         3.0      2.0\n",
      "16.0      1.0   1.0         1.0      NaN\n",
      "                          treat  control\n",
      "age_group alone sex_male                \n",
      "0.0       0.0   0.0         9.0     20.0\n",
      "                1.0        11.0     21.0\n",
      "          1.0   0.0         NaN      1.0\n",
      "1.0       0.0   0.0        16.0     12.0\n",
      "                1.0         6.0     15.0\n",
      "          1.0   0.0         5.0     12.0\n",
      "                1.0         9.0     27.0\n",
      "2.0       0.0   0.0        24.0     14.0\n",
      "                1.0        18.0     16.0\n",
      "          1.0   0.0        17.0     17.0\n",
      "                1.0        28.0     86.0\n",
      "3.0       0.0   0.0        22.0     10.0\n",
      "                1.0        22.0      8.0\n",
      "          1.0   0.0        22.0      6.0\n",
      "                1.0        32.0     45.0\n",
      "4.0       0.0   0.0        13.0      8.0\n",
      "                1.0        13.0      6.0\n",
      "          1.0   0.0        10.0      1.0\n",
      "                1.0        19.0     19.0\n",
      "5.0       0.0   0.0        11.0      NaN\n",
      "                1.0         8.0      NaN\n",
      "          1.0   0.0         7.0      NaN\n",
      "                1.0        16.0      6.0\n",
      "6.0       0.0   0.0         2.0      NaN\n",
      "                1.0         4.0      NaN\n",
      "          1.0   0.0         1.0      1.0\n",
      "                1.0         9.0      2.0\n",
      "7.0       0.0   1.0         1.0      NaN\n",
      "          1.0   1.0         3.0      2.0\n",
      "8.0       1.0   1.0         1.0      NaN\n",
      "                          treat  control\n",
      "age_group alone sex_male                \n",
      "0.0       0.0   0.0        12.0     24.0\n",
      "                1.0        12.0     26.0\n",
      "          1.0   0.0         NaN      3.0\n",
      "                1.0         NaN      1.0\n",
      "1.0       0.0   0.0        37.0     22.0\n",
      "                1.0        23.0     26.0\n",
      "          1.0   0.0        22.0     27.0\n",
      "                1.0        37.0    112.0\n",
      "2.0       0.0   0.0        29.0     14.0\n",
      "                1.0        28.0     14.0\n",
      "          1.0   0.0        29.0      6.0\n",
      "                1.0        38.0     57.0\n",
      "3.0       0.0   0.0        17.0      4.0\n",
      "                1.0        15.0      NaN\n",
      "          1.0   0.0        10.0      1.0\n",
      "                1.0        29.0     13.0\n",
      "4.0       0.0   0.0         2.0      NaN\n",
      "                1.0         5.0      NaN\n",
      "          1.0   0.0         1.0      1.0\n",
      "                1.0        12.0      4.0\n",
      "5.0       1.0   1.0         1.0      NaN\n"
     ]
    }
   ],
   "source": [
    "# CEM\n",
    "exact_cols = ['age_group', 'alone','sex_male']\n",
    "for age_diff in [2,5,10,15]:\n",
    "    X['age_group'] =  X.age // age_diff\n",
    "    coarse = pd.DataFrame({'treat':X[D].groupby(exact_cols).size(), \n",
    "                  'control':X[~D].groupby(exact_cols).size()})\n",
    "    print(coarse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continue with age difference = 5.\n",
    "\n",
    "> **Ex. 4.1.3:** Compute the average treatment effect by using (coarsened) exact matching on `age` (i.e. on `age_group`). \n",
    ">\n",
    ">Comment on the result. How does the group treatment effects compare to the ATE you found in 4.1.2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here\n",
    "cols = ['age_group', 'treated']\n",
    "X['age_group'] =  X.age // 5\n",
    "coarse = X.copy()\n",
    "coarse['treated'] = D.astype(int)\n",
    "coarse.set_index(cols, inplace=True)\n",
    "coarse = coarse['survived']\n",
    "y_coarse = coarse.groupby(cols).mean().unstack('treated')\n",
    "ate = y_coarse[1] - y_coarse[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3597437016811178"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ate.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.4:** Estimate a logistic regression model for predicting the passenger class variable (i.e. `D`, the treatment indicator)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.876750700280112"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your answer here\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "# deselect highly correlated columns (age, age_group) (man, woman)\n",
    "cols = ['age', 'sibsp', 'parch', 'fare', 'adult_male', 'alone',\n",
    "       'sex_male', 'embarked_Q', 'embarked_S', 'deck_B', 'deck_C', 'deck_D', 'deck_E', 'deck_F', 'deck_G',\n",
    "       'embark_town_Queenstown', 'embark_town_Southampton']\n",
    "\n",
    "# lr with lasso to better deal with the 20 dimensions\n",
    "clf = LogisticRegression(random_state=0, max_iter=1000, n_jobs=-1, penalty='l2').fit(X[cols], D)\n",
    "X['prop'] = clf.predict_proba(X[cols])[:,0]\n",
    "clf.score(X[cols], D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.5:** What other models might we have chosen? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are moddeling a binary variable we could instead have used treebased models such as a random forrest. Alternatively a distance based method like k-nearest neighbors or support vector machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tobias\\Miniconda3\\envs\\sds_eml_2020\\lib\\site-packages\\sklearn\\svm\\_base.py:947: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC: 0.8291316526610645, RFC: 1.0, Neighbors: 0.9355742296918768\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "svc = LinearSVC(C=1.0, max_iter=1000).fit(X[cols], D)\n",
    "rfc = RandomForestClassifier(n_jobs=-1).fit(X[cols], D)\n",
    "nbrs = KNeighborsClassifier().fit(X[cols], D)\n",
    "\n",
    "print(f\"SVC: {svc.score(X[cols], D)}, RFC: {rfc.score(X[cols], D)}, Neighbors: {nbrs.score(X[cols], D)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.6:** What is the overlap of predicted probabilities? What happens if you estimate the model without `fare` and `deck`? Comment\n",
    ">\n",
    "> Why do `fare` and `deck` matter a lot in this setting, try to draw a causal diagram that might illuminate your discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6932773109243697"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your answer here\n",
    "cols2 = ['age', 'sibsp', 'parch', 'adult_male', 'alone',\n",
    "       'sex_male', 'embarked_Q', 'embarked_S',\n",
    "       'embark_town_Queenstown', 'embark_town_Southampton']\n",
    "\n",
    "# lr with lasso to better deal with the 20 dimensions\n",
    "clf = LogisticRegression(random_state=0, max_iter=1000, n_jobs=-1, penalty='l2').fit(X[cols2], D)\n",
    "clf.score(X[cols2], D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How much they paid and which deck they were on is highly correlated with which class they where on, because: \n",
    " * i) higher class tickets are more expensive in general, and \n",
    " * ii) lower class cabins are often placed differently from the highter ones (e.g lower in ship)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.7:** Use a 5-nearest-neighbors matching in propensity space to compute the average treatment effect. Bootstrap the 95 pct. confidence interval of the ATE. What happens if you select only propensity score values with high common support, i.e. between 0.2 and 0.8?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      1.0\n",
       "385    0.0\n",
       "445    1.0\n",
       "72     0.0\n",
       "540    1.0\n",
       "Name: alive, dtype: float64"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your answer here\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "nn = NearestNeighbors(n_neighbors=5, n_jobs=-1)\n",
    "nn.fit(propensity)  # propenbsity calculated from the first LR\n",
    "indicies = nn.kneighbors(propensity[D],n_neighbors=5, return_distance=False)\n",
    "y.iloc[indicies[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex. 4.1.7:** (BONUS) How might we improve on the approach above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(714, 22)"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Your answer here\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Honest trees\n",
    "\n",
    "In this problem we will try to implement and understand some of the ideas used in [Athey, Imbens (2015)](https://www.pnas.org/content/pnas/113/27/7353.full.pdf) to develop _Honest Inference_ in desicion tree models. The paper begins by covering honesty in a setting of population averages, and for estimating conditional means; so you will need to look towards the second half of the paper to get an impression of it's use for treatment-effect estimation.\n",
    "\n",
    "> **Ex. 4.2.1:** What does it mean that a tree is _honest?_ In particular what are the implications in terms of \n",
    "> * The intuition for why honesty is required in order to get good local treatment effect estimates?\n",
    "> * The practical implementation of the DT algorithm?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex.4.2.2:** Use the `load_42_data` function to load the boston house-price dataset. Split your dataset in two. A 50% test set and a 50% train set using `sklearn.model_selection.train_test_split`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_42_data():\n",
    "    from sklearn.datasets import load_boston\n",
    "    df = load_boston()\n",
    "    df = pd.DataFrame(np.c_[df['data'], df['target']], columns = list(df['feature_names']) + ['y'])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex 4.2.3:** Identify the column and value in `X_train` that minimizes the (cross split weighted) sum of squared errors in the training data. Split the test data according to this value and report the mean and standard deviation of `y` in both subsamples for both the train and test data.\n",
    ">\n",
    "> Comment on your results. How different are the two subsamples from the overall mean and standard deviation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **Ex 4.2.4:** Redo your analysis from 4.2.3, but this time split in a 66% train dataset and a 33% test dataset. Split the train data once more 50/50 to get a train and an estimation dataset. \n",
    ">\n",
    "> Focus only on one of the subsets (i.e. either the left or right leaf). \n",
    ">\n",
    "> Report the same statistics as before, but for train, estimation and test samples. This time, show your results as density plots graphing 5.000 bootstrap replications of the whole procedure. If your pc is slow, you might need to reduce the number of replications to 1000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
